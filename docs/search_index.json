[["index.html", "A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer Chapter 1 Introduction", " A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer Ji Qi, Xingye Feng 2022-12-15 Chapter 1 Introduction "],["proposal.html", "Chapter 2 Proposal 2.1 Research topic 2.2 Data availability", " Chapter 2 Proposal 2.1 Research topic Our research topic is: A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer. We are interested in this topic because it is a real-world application of exploratory data visualization/analyze which can solve real puzzles that the company has. In the food packaging manufacturing industry, there are tens of thousands of pounds of finished good rejected due to quality defect in a single manufacturing plant every day. In the meantime, there are thousands of quality inspection data points generated in a single plant each day. If those data are simply collected but not analyzed, they have no value on process improvement or efficiency enhancement. Thus, through our project, we would like to provide a value-added study by analyzing the Quality Exception data and eventually lead the way to optimization and improvement for the manufacturing plant. To give an example, among all the production lines and hundreds of products produced each day, which feature of which product need special focus to improve its pack out efficiency (meaning to have less reject)? What are the inspection features that are strongly related to each other so they can be reviewed together for quality improvement? 2.2 Data availability Our data are all sourced from Sabert Corporation, a leading food packaging manufacturing company. We are interested in the manufacturing Quality Exception data in the New Jersey plant. The data will be provided directly by its Quality Assurance department. Group member Xingye works there and is responsible of collecting the data. Since the data is not public, there is no direct link to the data source. However, it will be downloaded by Xingye from Sabert’s Intranet database. Sabert uses a Statistical Process Control system for Quality Inspection data collection. All Quality Inspection data are stored on a general database. We are interested in the Exception data specifically of all data, which can be exported as a .csv file from the general data base. We plan to import the .csv data into R and perform data visualization and analysis. The Quality Inspection data we are interested is collected by Quality Inspectors at Sabert New Jersey plant, on a 12-hour shift, entered real-time. The plant has 13 manufacturing lines, Quality Inspectors enter inspection data two to three times each shift for each line. The system generates an automatic report of the Exception data every 24 hour. We will study the data for the past 6 months, which is a reasonable time frame in manufacturing to identify long term issue and analyze patterns. The Quality Inspection data includes the numerical and categorical data. For example, numerical data includes weight, wall thickness, color index etc. Categorical data results are pass or fail which includes visual inspection, form inspection, perforation inspection etc. We will study the data through visualization and find recommendation models that reduce exception rate. In order to import the data, we need to determine the date range we are interested in, export all exception data in a .csv file, and open in R to start analyzing. If we have question about the data, we will contact the Quality Manager at Sabert New Jersey plant directly to get an understanding and address the questions. Since it is a manufacturing data set, we expect that it will require data cleaning, duplicate dropping, outlier excluding in some extent. We will study and sanitize the data set first before starting the analyzation. Sources: [Sabert Corporation Website] (https://sabert.com/) "],["data.html", "Chapter 3 Data 3.1 Sources 3.2 Cleaning / transformation 3.3 Missing value analysis", " Chapter 3 Data 3.1 Sources Our data is retrieved from Sabert internal Statistical Process Control (SPC) system. We choose to study the daily product exception data, which is data for products that are out of specification or rejected by Quality technicians. Data was collected by the Quality technicians on a daily basis, at least four times per shift, and entered into the computer data collection system SPC. Based on our study, there are two general types of data collected in the system – variables and attributes. Variables include Part Weight, Silicone Ratio, Wall Thickness Top, Wall Thickness Bottom, Haze, and Color which are measurable in numeric format. Attributes include Visual Evaluation, Form, Registration, Angel Hair, Perforation Test, Lid Fit, Label Test, Antifog Test, etc. Which are evaluated by personal judgment and deemed as pass or fail. The exception dataset we are studying includes only reject data for Attributes, or out-of-specification range data for Variables. We choose those data because they are direct measurements and indicators of defective products that are produced in the manufacturing plant every day. Studying those data can help the Quality department find significant defect patterns, unreliable machine processes, and potential correlations between different Variables/Attributes to tackle stubborn issues. We picked three months of production Quality data for our research. This includes 27,187 Exception observations. We believe this is a substantial enough data set after evaluating the diversity of the data, consulting the employees in the company, and learning the product and manufacturing features of the plant. The data include the following key information we are interested in: Machine Line #, Item/Material #, Item/Material description, MIC Characteristic Description, Sample#, Result, Lower Specification Limit, Target, Upper Specification Limit, Posting Date, and Inspection Plan. In the raw data set, each row is an Exception observation with the above information listed as columns. Although the data set has a tidy data feature where every column is a variable and every row is an observation, since it contains too many columns, it is important to select the variables we would study, which are supposed to be value-added. There is also an opportunity to drop duplicates since many rows contain the same information. We will discuss that in the section below. 3.2 Cleaning / transformation First, we found it has duplicates in the Attributes data. So we dropped the duplicate rows of the same machine, same item, and same time of Attributes data, because their results are all the same “reject” so there is no value to include all of them. Secondly, in order to analyze the Variables and Attributes Exception data independently, we separated the data into two data frames - Variable Dataframe -&gt; dfn_variable and Attribute Dataframe -&gt; dfn_attribute. Thirdly, we study variables by doing the operation (Result – Target)/Target to calculate the difference ratio of an observation result versus target. This is because, for example, products in the company have very different weight targets intrinsically, if simply plotting all product part weight results together, they will have ununiformed scales, and will be incomparable or meaningless to compare. So it is essential to rescale the results, into a difference ratio so that we are retrieving one product’s results that differ from its own target and compare all items after they are standardized. 3.3 Missing value analysis While drawing the missing value graphs, we found that there are three types of missing values in the dataset.In the Attribute data frame, all values in the Target column are NA, because there is not a real target for it. So we choose to fill the column with 0. In the Variable data frame, there are some rows without value in Target, those are the characteristics without target value and their appearance is very trivial in the dataset, so we chose to drop those rows. Also, we found the LSL and USL columns have missing values, since we do not study those features, we chose to drop the two columns completely. ## Target LSL USL WorkcenterDesc ## 0.2256 0.1422 0.0834 0.0000 ## Material_Code Material_Desc MIC_Desc SampleSize ## 0.0000 0.0000 0.0000 0.0000 ## SampleNumber Result MICUoM PostingDate ## 0.0000 0.0000 0.0000 0.0000 ## UserName InspectionComments InspectionPlan COAComments ## 0.0000 0.0000 0.0000 0.0000 ## LSL Target USL WorkcenterDesc ## 100 100 100 0 ## Material_Code Material_Desc MIC_Desc SampleSize ## 0 0 0 0 ## SampleNumber Result MICUoM PostingDate ## 0 0 0 0 ## UserName InspectionComments InspectionPlan COAComments ## 0 0 0 0 ## WorkcenterDesc Material_Code Material_Desc MIC_Desc ## 0 0 0 0 ## SampleSize SampleNumber Result MICUoM ## 0 0 0 0 ## Target PostingDate UserName InspectionComments ## 0 0 0 0 ## InspectionPlan COAComments ## 0 0 "],["results.html", "Chapter 4 Results 4.1 Bar Chart for All Machines to identify the top 2 machines with the most defect items 4.2 Group bar chart to identify the top attribute and varibales Among all machines 4.3 Parallel coordinate plot: count(partweight) vs count(wall thickness) vs count(silicone ratio) + count(all) every line summary 4.4 Ridgeline plot for part weight exception for each line: use weight difference = (result - target)/target. 4.5 Multiple boxplots of Part Weight Result according to WorkcenterDesc Type 4.6 Pick machine I4 and I7 to study because they have most exceptions.", " Chapter 4 Results 4.1 Bar Chart for All Machines to identify the top 2 machines with the most defect items 4.2 Group bar chart to identify the top attribute and varibales Among all machines 4.3 Parallel coordinate plot: count(partweight) vs count(wall thickness) vs count(silicone ratio) + count(all) every line summary 4.4 Ridgeline plot for part weight exception for each line: use weight difference = (result - target)/target. 4.5 Multiple boxplots of Part Weight Result according to WorkcenterDesc Type 4.6 Pick machine I4 and I7 to study because they have most exceptions. 4.6.1 Study Machine I4 4.6.2 Study Machine I7 4.6.3 Time series plot of attribute exception throughout the 3 months. For I4 and I7 repectively. (overall + by week) 4.6.4 Heatmap 4.6.5 Scatter Plot ## # A tibble: 160 × 5 ## # Groups: WorkcenterDesc [2] ## WorkcenterDesc date weight silicone month ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 I4 0022-09-01 -0.0356 NaN September ## 2 I4 0022-09-02 0.0437 NaN September ## 3 I4 0022-09-03 0.0834 0.00118 September ## 4 I4 0022-09-04 0.0773 NaN September ## 5 I4 0022-09-05 0.0702 -0.000539 September ## 6 I4 0022-09-06 0.0270 -0.000539 September ## 7 I4 0022-09-07 -0.00788 NaN September ## 8 I4 0022-09-08 -0.0286 NaN September ## 9 I4 0022-09-09 0.0297 NaN September ## 10 I4 0022-09-10 0.0559 NaN September ## # … with 150 more rows 4.6.6 Cleveland dot plot with multiple dots "],["interactive-component.html", "Chapter 5 Interactive component", " Chapter 5 Interactive component "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
