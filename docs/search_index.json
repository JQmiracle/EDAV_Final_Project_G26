[["index.html", "A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer Chapter 1 Introduction", " A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer Ji Qi, Xingye Feng 2022-12-15 Chapter 1 Introduction "],["proposal.html", "Chapter 2 Proposal 2.1 Research topic 2.2 Data availability", " Chapter 2 Proposal 2.1 Research topic Our research topic is: A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer. We are interested in this topic because it is a real-world application of exploratory data visualization/analyze which can solve real puzzles that the company has. In the food packaging manufacturing industry, there are tens of thousands of pounds of finished good rejected due to quality defect in a single manufacturing plant every day. In the meantime, there are thousands of quality inspection data points generated in a single plant each day. If those data are simply collected but not analyzed, they have no value on process improvement or efficiency enhancement. Thus, through our project, we would like to provide a value-added study by analyzing the Quality Exception data and eventually lead the way to optimization and improvement for the manufacturing plant. To give an example, among all the production lines and hundreds of products produced each day, which feature of which product need special focus to improve its pack out efficiency (meaning to have less reject)? What are the inspection features that are strongly related to each other so they can be reviewed together for quality improvement? 2.2 Data availability Our data are all sourced from Sabert Corporation, a leading food packaging manufacturing company. We are interested in the manufacturing Quality Exception data in the New Jersey plant. The data will be provided directly by its Quality Assurance department. Group member Xingye works there and is responsible of collecting the data. Since the data is not public, there is no direct link to the data source. However, it will be downloaded by Xingye from Sabert’s Intranet database. Sabert uses a Statistical Process Control system for Quality Inspection data collection. All Quality Inspection data are stored on a general database. We are interested in the Exception data specifically of all data, which can be exported as a .csv file from the general data base. We plan to import the .csv data into R and perform data visualization and analysis. The Quality Inspection data we are interested is collected by Quality Inspectors at Sabert New Jersey plant, on a 12-hour shift, entered real-time. The plant has 13 manufacturing lines, Quality Inspectors enter inspection data two to three times each shift for each line. The system generates an automatic report of the Exception data every 24 hour. We will study the data for the past 6 months, which is a reasonable time frame in manufacturing to identify long term issue and analyze patterns. The Quality Inspection data includes the numerical and categorical data. For example, numerical data includes weight, wall thickness, color index etc. Categorical data results are pass or fail which includes visual inspection, form inspection, perforation inspection etc. We will study the data through visualization and find recommendation models that reduce exception rate. In order to import the data, we need to determine the date range we are interested in, export all exception data in a .csv file, and open in R to start analyzing. If we have question about the data, we will contact the Quality Manager at Sabert New Jersey plant directly to get an understanding and address the questions. Since it is a manufacturing data set, we expect that it will require data cleaning, duplicate dropping, outlier excluding in some extent. We will study and sanitize the data set first before starting the analyzation. Sources: [Sabert Corporation Website] (https://sabert.com/) "],["data.html", "Chapter 3 Data 3.1 Sources 3.2 Cleaning / transformation 3.3 Missing value analysis", " Chapter 3 Data 3.1 Sources Our data is retrieved from Sabert internal Statistical Process Control (SPC) system. We choose to study the daily product exception data, which is data for products that are out of specification or rejected by Quality technicians. Data was collected by the Quality technicians on a daily basis, at least four times per shift, and entered into the computer data collection system SPC. Based on our study, there are two general types of data collected in the system – variables and attributes. Variables include Part Weight, Silicone Ratio, Wall Thickness Top, Wall Thickness Bottom, Haze, and Color which are measurable in numeric format. Attributes include Visual Evaluation, Form, Registration, Angel Hair, Perforation Test, Lid Fit, Label Test, Antifog Test, etc. Which are evaluated by personal judgment and deemed as pass or fail. The exception dataset we are studying includes only reject data for Attributes, or out-of-specification range data for Variables. We choose those data because they are direct measurements and indicators of defective products that are produced in the manufacturing plant every day. Studying those data can help the Quality department find significant defect patterns, unreliable machine processes, and potential correlations between different Variables/Attributes to tackle stubborn issues. We picked three months of production Quality data for our research. This includes 27,187 Exception observations. We believe this is a substantial enough data set after evaluating the diversity of the data, consulting the employees in the company, and learning the product and manufacturing features of the plant. The data include the following key information we are interested in: Machine Line #, Item/Material #, Item/Material description, MIC Characteristic Description, Sample#, Result, Lower Specification Limit, Target, Upper Specification Limit, Posting Date, and Inspection Plan. In the raw data set, each row is an Exception observation with the above information listed as columns. Although the data set has a tidy data feature where every column is a variable and every row is an observation, since it contains too many columns, it is important to select the variables we would study, which are supposed to be value-added. There is also an opportunity to drop duplicates since many rows contain the same information. We will discuss that in the section below. 3.2 Cleaning / transformation First, we found it has duplicates in the Attributes data. So we dropped the duplicate rows of the same machine, same item, and same time of Attributes data, because their results are all the same “reject” so there is no value to include all of them. Secondly, in order to analyze the Variables and Attributes Exception data independently, we separated the data into two data frames - Variable Dataframe -&gt; dfn_variable and Attribute Dataframe -&gt; dfn_attribute. Thirdly, we study variables by doing the operation (Result – Target)/Target to calculate the difference ratio of an observation result versus target. This is because, for example, products in the company have very different weight targets intrinsically, if simply plotting all product part weight results together, they will have ununiformed scales, and will be incomparable or meaningless to compare. So it is essential to rescale the results, into a difference ratio so that we are retrieving one product’s results that differ from its own target and compare all items after they are standardized. 3.3 Missing value analysis While drawing the missing value graphs, we found that there are three types of missing values in the dataset.In the Attribute data frame, all values in the Target column are NA, because there is not a real target for it. So we choose to fill the column with 0. In the Variable data frame, there are some rows without value in Target, those are the characteristics without target value and their appearance is very trivial in the dataset, so we chose to drop those rows. Also, we found the LSL and USL columns have missing values, since we do not study those features, we chose to drop the two columns completely. 3.3.1 Check Missing Values of Variable Data before data cleaning ## Target LSL USL WorkcenterDesc ## 0.2256 0.1422 0.0834 0.0000 ## Material_Code Material_Desc MIC_Desc SampleSize ## 0.0000 0.0000 0.0000 0.0000 ## SampleNumber Result MICUoM PostingDate ## 0.0000 0.0000 0.0000 0.0000 ## UserName InspectionComments InspectionPlan COAComments ## 0.0000 0.0000 0.0000 0.0000 3.3.2 Check Missing Values of Variable Data after data cleaning 3.3.3 Check Missing Values of Attribute Data before data cleaning ## LSL Target USL WorkcenterDesc ## 100 100 100 0 ## Material_Code Material_Desc MIC_Desc SampleSize ## 0 0 0 0 ## SampleNumber Result MICUoM PostingDate ## 0 0 0 0 ## UserName InspectionComments InspectionPlan COAComments ## 0 0 0 0 3.3.4 Check Missing Values of Attribute Data after data cleaning "],["results.html", "Chapter 4 Results 4.1 Bar Chart for All Machines to identify the top 2 machines with the most defect items 4.2 Group bar chart to identify the top attribute and varibales Among all machines 4.3 Parallel coordinate plot: 4.4 Multiple boxplots of Part Weight Result according to WorkcenterDesc Type 4.5 Ridgeline plot for part weight exception for each line: use weight difference = (result - target)/target. 4.6 Pick machine I4 and I7 to study because they have most exceptions.", " Chapter 4 Results 4.1 Bar Chart for All Machines to identify the top 2 machines with the most defect items Since there are thirteen machines in the factory, we are interested in checking if there is distribution difference of Exception data among them. Meaning that if there are some machines that have apparently more exceptions than the others. From the histograms above, we found that machine I4 and machine I7 are the two machines that have the most Exception frequency for both Variables and Attributes characteristics. So we will further study those machines in depth later. It also gives us a rough idea that there might be correlation between Variable and Attribute exception performances, which means if a machine has more Variable exception, it could have more Attribute exception potentially. We will study that later as well. 4.2 Group bar chart to identify the top attribute and varibales Among all machines In a general review of Attribute and Variable data by machine, we found that Visual Evaluation and Form are the major Attribute exceptions for most of the machines except for I1. They take up at least 60% of the Attributes exceptions. This is a good information for the company to assign attention and resources based on the category of defects. For example, in order to address the Visual Evaluation exceptions, it may be good to focus on the quality of raw materials used in production. Part Weight is the most significant Variable exception for all machines, with Silicone Ratio being the secondary. In the following sections, we will put Part Weight and Silicone Ratio as key focuses to study. 4.3 Parallel coordinate plot: Remember we found the in the beginning that Variable and Attribute Exceptions may have correlations between each other. So we picked the top two Variable and Attribute Characteristics to plot the Parallel Coordinate Plot based on the incident count of each of them by machine. We found that Part Weight have positive correlation with Visual Evaluation. Visual Evaluation have positive correlation with Form. Silicone do not have strong relationship with other characteristics. This tells us that there are strong correlation between the top two Attribute Exceptions themselves. And Part Weight, the most significant Variable Exception, has strong correlation with the top Attribute Exception - Visual Evaluation. So it validates our assumption. That means, if there is a unstable process in any machine, it is likely that it will affect multiple characteristics of the item in a negative way. So a red flag should be raised during Quality inspections that if one of the above characteristics is in exception, the others should be carefully checked as well. 4.4 Multiple boxplots of Part Weight Result according to WorkcenterDesc Type From the Box plots of the part weight data by machine above, we found that there are some machines that run a wider range of part weights. For example, machine I5 and I8. That means the items run on those machines can be varied from small to large sizes. In the opposite, some machines like I9 and I2 run smaller scale of items which have closer weights. This plot also tells us it is important to study the Part weight difference ratio from target, which eliminates the impact of the original small or large part weight of the items themselves. And focus on the difference of the part weight with their target to study the level of out-of-range. 4.5 Ridgeline plot for part weight exception for each line: use weight difference = (result - target)/target. In order to study the difference ratio of Part Weight from Target, we define it by the equation (Result - Target)/Target, and plot that result using the ridge line plot by machine. The Plot above tells us I9 has more out of range Part Weight on the upper side, meaning that they run more over weight. Which is not cost effective to the company. And machine I6, I5, I13, I1 tend to run out of range on the lower bound. So it is important to pay attention to those lines that the quality of products are not compromised because of the low weight. 4.6 Pick machine I4 and I7 to study because they have most exceptions. Next, after we study the different patterns on all machines, we would like to take the two machines that we picked in the beginning, I4 and I7, to deep dive into them and find out more valuable information. 4.6.1 Heatmap There are three different types of inspection, which are inspections conducted by different group of people. We would like to use heatmaps to find out relationship of inspections conducted by different people. The first two graphs represents that relationship of two machines I7 and I4. As shown in the heat map, the part weight collected by different group of people for the same item do not show exact same pattern. They are somewhat similar on color, however, not all items are perfectly the same. Our recommendation for this is to control and align the data collection methods used by different groups of people, in order to make sure data collected are accurate and reflect consistence. The third and fourth graphs are to examine the relationship among Variable Exceptions for I7 and I4, where we do not find very similar patterns among those different Variable characteristics. 4.6.2 Scatter Plot In order to further study if there is relationship between Part Weight and Silicone Ratio for I4 and I7, we used scatter plots. On I4 there is a weak positive correlation between, however I7 has no strong relationship. This shows that part weight and silicone ratio are not correlated to each other, though they are the top two Variable exceptions. 4.6.3 Time series plot of attribute exception throughout the 3 months. For I4 and I7 repectively. (overall + by weekday trend) Next we would like to study if time has any impact on product performance. So we use time series plot to explore if pattern exists. For machine I4, the first graph is an overview of three month time series. We mark out the weekends with red dot. To see if the more fluctuation exist on weekends comparing to weekdays. The first plot does show the red dots fall on high or low peaks, which validate our assumption. The second graph group the data by days in week and we observe on Saturday, there are more extreme values and wider range. This shows us on machine I4, due to people resource limitation on weekends, the weight exception is more extreme than other days. We recommend the Quality department to pay higher attention on weekends on product and process stability, to avoid quality defects. We used the same idea to study weekend performance for machine I7, it does not show the same conclusion as machine I4. So that means the people factor is less affected by the day in week on I7. 4.6.4 Cleveland dot plot with multiple dots To further dive into to the Part Weight performance for I4, we used Cleveland dot plot to draw the rank of each item run on this machine by Part Weight difference ratio. Based on this graph, we recommend red flagging the top 7 items and low 6 items in the graph on part weight control in order to achieve better part weight performance for the machine overall. Use the same logic as above and based on this graph, we recommend red flagging the top 8 items and low 7 items in the graph on part weight control in order to achieve better part weight performance for machine I7 overall. Also in the above two graphs, we see that for items that show up in more than two months, the weight of them tend to increase by time, which shows it could potentially be affected by the tool of the item being fatigue and require maintenance over time. "],["interactive-component.html", "Chapter 5 Interactive component 5.1 New Interactive Features: 5.2 Instructions for users:", " Chapter 5 Interactive component 5.1 New Interactive Features: In order to have a better understanding of overall long-term trend of Average Part Weight among products inspected by Machine I4, our team will use d3.js to add a interactive feature, showing the Date and Average Part Weight Difference Ratio from Target for each point along the line. For d3.js part, we firstly create a tooltip and attach the appropriate Mouseover, Mousemove, and Mouseout event functions to dynamically move and change the visibility of a tooltip and help display the data values as the text. That newly added feature will help the user obtain the exact value of each data point without going back to check the data table anymore, and it could draw user’s attention while providing additional information. 5.2 Instructions for users: When the user moves the mouse pointer over the point along the line, the Date and Average Part Weight Difference Ratio from Target values will be displayed near the point in a black box. In addition, nothing will show on the graph when the mouse is away from the any point on the line. .tooltip { position: absolute; pointer-events: none; background: #000; color: #fff; padding: 10px; border-radius: 10px; font-size: 12px; } Average Part Weight Difference Ratio from Target of Each Product Inspected by Machine I4 "],["conclusion.html", "Chapter 6 Conclusion 6.1 Takeaways 6.2 Limitation and Further Steps", " Chapter 6 Conclusion 6.1 Takeaways We were glad to be able to make multiple recommendations for the company’s Quality department based on our exploratory data analysis. We also learned that different graphs can present very diverse information even based on the same data. And we believe the recommendation based on this true and time-effective data can make a valuable impact that helps the company to continuously improve. Here is a snap of our recommendations retrieved from section 4: If a machine has more Variable exceptions, it could have more Attribute exceptions potentially. So a red flag should be raised during Quality inspections if any characteristics are running abnormally. The main focus should be on part weight, silicone ratio, visual evaluation, and form, as they are the top two exceptions in Variables and Attributes respectively. For machines I9, I3, I4 and I11, the company should focus more on heavy weight issues. For machines I6, I5, I13, and I1 the company should focus more on the low weight issues. It is important to control and align the data collection methods used by different groups of people, this is in order to make sure the data collected are accurate and reflect consistency. For machine I4 We recommend the Quality department pay higher attention on weekends to product and process stability, to avoid quality defects. We have made specific Item recommendations for I4 and I7 to look out for weights. 6.2 Limitation and Further Steps There are some limitations of this EDA project. First of all, 3 month data couldn’t reflect the 100% of the real world situation. Especially, after dropping 2% of rows and 2 columns, the dataset lost several important features and information. Thus, we need to collect more data (at least 1 year) for the further analysis. In addition, given that this dataset only contains the rows with the ‘Defects’ items classified mainly by the human beings and there is still some probability of making mistakes incurred by human beings, it would be better to include the rows without ‘Defects’ items which could be used as a another reference for double-checking the human’s classifications using the numeric features. After implementing the recommendations mentioned in the takeways, we could collect the new data and repeat the same visualization procedure to see whether there is some performance improvement on those different machine lines. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
