--- 
title: "A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer"
author: "Ji Qi, Xingye Feng "
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---
```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Introduction


<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Proposal

## Research topic

Our research topic is: A Study of Manufacturing Quality Exception Data and the Patterns Behind of a Food Packaging Manufacturer. We are interested in this topic because it is a real-world application of exploratory data visualization/analyze which can solve real puzzles that the company has. In the food packaging manufacturing industry, there are tens of thousands of pounds of finished good rejected due to quality defect in a single manufacturing plant every day. In the meantime, there are thousands of quality inspection data points generated in a single plant each day. If those data are simply collected but not analyzed, they have no value on process improvement or efficiency enhancement. Thus, through our project, we would like to provide a value-added study by analyzing the Quality Exception data and eventually lead the way to optimization and improvement for the manufacturing plant. To give an example, among all the production lines and hundreds of products produced each day, which feature of which product need special focus to improve its pack out efficiency (meaning to have less reject)? What are the inspection features that are strongly related to each other so they can be reviewed together for quality improvement?

## Data availability

Our data are all sourced from Sabert Corporation, a leading food packaging manufacturing company. We are interested in the manufacturing Quality Exception data in the New Jersey plant. The data will be provided directly by its Quality Assurance department. Group member Xingye works there and is responsible of collecting the data. Since the data is not public, there is no direct link to the data source. However, it will be downloaded by Xingye from Sabert’s Intranet database. 

Sabert uses a Statistical Process Control system for Quality Inspection data collection. All Quality Inspection data are stored on a general database. We are interested in the Exception data specifically of all data, which can be exported as a .csv file from the general data base. We plan to import the .csv data into R and perform data visualization and analysis. 

The Quality Inspection data we are interested is collected by Quality Inspectors at Sabert New Jersey plant, on a 12-hour shift, entered real-time. The plant has 13 manufacturing lines, Quality Inspectors enter inspection data two to three times each shift for each line. The system generates an automatic report of the Exception data every 24 hour. We will study the data for the past 6 months, which is a reasonable time frame in manufacturing to identify long term issue and analyze patterns. 

The Quality Inspection data includes the numerical and categorical data. For example, numerical data includes weight, wall thickness, color index etc. Categorical data results are pass or fail which includes visual inspection, form inspection, perforation inspection etc. We will study the data through visualization and find recommendation models that reduce exception rate. 

In order to import the data, we need to determine the date range we are interested in, export all exception data in a .csv file, and open in R to start analyzing.

If we have question about the data, we will contact the Quality Manager at Sabert New Jersey plant directly to get an understanding and address the questions.

Since it is a manufacturing data set, we expect that it will require data cleaning, duplicate dropping, outlier excluding in some extent. We will study and sanitize the data set first before starting the analyzation. 

**Sources:**
[Sabert Corporation Website] (https://sabert.com/)



<!--chapter:end:proposal.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Data 

## Sources
Our data is retrieved from Sabert internal Statistical Process Control (SPC) system. We choose to study the daily product exception data, which is data for products that are out of specification or rejected by Quality technicians. Data was collected by the Quality technicians on a daily basis, at least four times per shift, and entered into the computer data collection system SPC.
Based on our study, there are two general types of data collected in the system – variables and attributes. Variables include Part Weight, Silicone Ratio, Wall Thickness Top, Wall Thickness Bottom, Haze, and Color which are measurable in numeric format. Attributes include Visual Evaluation, Form, Registration, Angel Hair, Perforation Test, Lid Fit, Label Test, Antifog Test, etc. Which are evaluated by personal judgment and deemed as pass or fail. The exception dataset we are studying includes only reject data for Attributes, or out-of-specification range data for Variables. We choose those data because they are direct measurements and indicators of defective products that are produced in the manufacturing plant every day. Studying those data can help the Quality department find significant defect patterns, unreliable machine processes, and potential correlations between different Variables/Attributes to tackle stubborn issues.
 
We picked three months of production Quality data for our research. This includes 27,187 Exception observations. We believe this is a substantial enough data set after evaluating the diversity of the data, consulting the employees in the company, and learning the product and manufacturing features of the plant.
 
The data include the following key information we are interested in: Machine Line #, Item/Material #, Item/Material description, MIC Characteristic Description, Sample#, Result, Lower Specification Limit, Target, Upper Specification Limit, Posting Date, and Inspection Plan. In the raw data set, each row is an Exception observation with the above information listed as columns. Although the data set has a tidy data feature where every column is a variable and every row is an observation, since it contains too many columns, it is important to select the variables we would study, which are supposed to be value-added. There is also an opportunity to drop duplicates since many rows contain the same information. We will discuss that in the section below.
 


## Cleaning / transformation

First, we found it has duplicates in the Attributes data. So we dropped the duplicate rows of the same machine, same item, and same time of Attributes data, because their results are all the same “reject” so there is no value to include all of them.

Secondly, in order to analyze the Variables and Attributes Exception data independently, we separated the data into two data frames - Variable Dataframe -> dfn_variable and Attribute Dataframe -> dfn_attribute.

Thirdly, we study variables by doing the operation (Result – Target)/Target to calculate the difference ratio of an observation result versus target. This is because, for example, products in the company have very different weight targets intrinsically, if simply plotting all product part weight results together, they will have ununiformed scales, and will be incomparable or meaningless to compare. So it is essential to rescale the results, into a difference ratio so that we are retrieving one product’s results that differ from its own target and compare all items after they are standardized.


```{r}
## Required Libraries
library(dplyr, warn.conflicts = FALSE)
library(gridExtra, warn.conflicts = FALSE)
library(ggridges, warn.conflicts = FALSE)
library(forcats, warn.conflicts = FALSE)
library(GGally, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)
library(ggplot2)
library(RColorBrewer)
library(lubridate)
library(grid)
library(lattice)
```


```{r}
# Import the dataset
Exception_data <- read.csv(file = 'Sep-Nov.22.csv')


# Drop duplicate attributes and rows with MIC_Desc = "Box ID Check"
dfn <- Exception_data[!duplicated(Exception_data[c("MIC_Desc","Result","PostingDate")]),] 
dfn <- dplyr::filter(dfn, MIC_Desc!="Box ID Check")
dfn <- dplyr::filter(dfn, MIC_Desc!="Bag ID Check")


#filter by attribute datatype
dfn_attribute <- dplyr::filter(dfn, Result=="Reject") 
#filter by variable datatype
dfn_variable <- dfn %>% 
    filter (Result != ("Inside")  &  Result != ("Reject"))


# Change the 'Result' datatype as numeric
dfn_variable$'Result' <- as.numeric(dfn_variable$'Result')
# Change the 'Target' datatype as numeric
dfn_variable$'Target' <- as.numeric(dfn_variable$'Target')


# Add a new column called 'weight_diff' by calibrate the Result of each item using (Result - Target)/Target 
dfn_weight <- dplyr::filter(dfn_variable, MIC_Desc =="Part Weight")
dfn_weight <- within(dfn_weight, weight_diff <- (Result - Target)/Target )
```



## Missing value analysis

While drawing the missing value graphs, we found that there are three types of missing values in the dataset.In the Attribute data frame, all values in the Target column are NA, because there is not a real target for it. So we choose to fill the column with 0.
In the Variable data frame, there are some rows without value in Target, those are the characteristics without target value and their appearance is very trivial in the dataset, so we chose to drop those rows. Also, we found the LSL and USL columns have missing values, since we do not study those features, we chose to drop the two columns completely.

```{r}
round(colSums(is.na(dfn_variable)) / nrow(dfn_variable) * 100,4)  %>%
  sort(decreasing = TRUE) 
```

```{r}
dfn_variable <- dfn_variable %>%
  select(-LSL, -USL)  

dfn_variable <- dfn_variable %>% drop_na()
  
```


```{r}
remotes::install_github("jtr13/redav")
library(redav)
plot_missing(dfn_variable, percent = FALSE)
```

```{r}
round(colSums(is.na(dfn_attribute)) / nrow(dfn_attribute) * 100,4)  %>%
  sort(decreasing = TRUE) 
```

```{r}
dfn_attribute <- dfn_attribute %>%
  select(-LSL, -USL)  

dfn_attribute <- dfn_attribute %>%
  fill(Target, .direction = 'up')
```

```{r}
dfn_attribute <- dfn_attribute %>%
  replace(is.na(.), 0)

round(colSums(is.na(dfn_attribute)) / nrow(dfn_attribute) * 100,4)  %>%
  sort(decreasing = TRUE) 
```



```{r}
remotes::install_github("jtr13/redav")
library(redav)
plot_missing(dfn_attribute, percent = FALSE)
```





<!--chapter:end:data.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Results


## Bar Chart for All Machines to identify the top 2 machines with the most defect items
```{r}
p1 <- dfn_variable%>%   
  group_by(WorkcenterDesc) %>% 
  tally()  %>% 
  ggplot(aes(x = fct_reorder(WorkcenterDesc, n, .desc = TRUE), y = n))+
  geom_bar(stat="identity")+
  labs(x = "Machine", y = "Freq", title = "Variable Exception Frequency by Machine")

p2 <- dfn_attribute%>%   
  group_by(WorkcenterDesc) %>% 
  tally()  %>% 
  ggplot(aes(x = fct_reorder(WorkcenterDesc, n, .desc = TRUE), y = n))+
  geom_bar(stat="identity")+
  labs(x = "Machine", y = "Freq", title = "Attribute Exception Frequency by Machine")


grid.arrange(p1, p2, ncol = 2)
```
Since there are thirteen machines in the factory, we are interested in checking if there is distribution difference of Exception data among them. Meaning that if there are some machines that have apparently more exceptions than the others. From the histograms above, we found that machine I4 and machine I7 are the two machines that have the most Exception frequency for both Variables and Attributes characteristics. So we will further study those machines in depth later. It also gives us a rough idea that there might be correlation between Variable and Attribute exception performances, which means if a machine has more Variable exception, it could have more Attribute exception potentially. We will study that later as well. 

## Group bar chart to identify the top attribute and varibales Among all machines
```{r}
p3<- dfn_attribute %>%   
  group_by(WorkcenterDesc, MIC_Desc) %>% 
  count(Result)%>% 
  group_by(WorkcenterDesc) %>% 
  mutate(Proportion = `n` / sum(n)) %>% 
  ggplot(aes(x = fct_reorder2(WorkcenterDesc, MIC_Desc, Proportion, .desc = FALSE), y = Proportion, fill = MIC_Desc))+
  geom_bar(position="fill", stat="identity")+
    labs(x = "Machine", y = "Proportion", title = "Top Attribute Exceptions for each machine")+
    coord_flip()+
scale_fill_brewer(palette="Paired")


p4<-dfn_variable %>%   
  group_by(WorkcenterDesc, MIC_Desc) %>% 
  tally()%>% 
  group_by(WorkcenterDesc) %>% 
  mutate(Proportion = `n` / sum(n)) %>% 
  ggplot(aes(x = fct_reorder2(WorkcenterDesc, MIC_Desc, Proportion, .desc = TRUE), y = Proportion, fill = MIC_Desc))+
  geom_bar(position="fill", stat="identity")+
    labs(x = "Machine", y = "Proportion", title = "Top Variable Exceptions for each machine")+
    coord_flip()+
scale_fill_brewer(palette="Paired")

p3
p4
```
In a general review of Attribute and Variable data by machine, we found that Visual Evaluation and Form are the major Attribute exceptions for most of the machines except for I1. They take up at least 60% of the Attributes exceptions. This is a good information for the company to assign attention and resources based on the category of defects. For example, in order to address the Visual Evaluation exceptions, it may be good to focus on the quality of raw materials used in production. 

Part Weight is the most significant Variable exception for all machines, with Silicone Ratio being the secondary. In the following sections, we will put Part Weight and Silicone Ratio as key focuses to study. 

## Parallel coordinate plot: 
```{r}
dfn_p_s <- dfn_variable %>%   
  filter(MIC_Desc == 'Part Weight' | MIC_Desc == 'Silicone Ratio' )

dfn_v_a <-
  rbind(dfn_p_s, dfn_attribute)

dfn_v_a %>%  
  group_by(WorkcenterDesc, MIC_Desc)%>%
  tally(sort = TRUE)  %>% pivot_wider(names_from = MIC_Desc, values_from = n)%>%   
  select(c('Part Weight', 'Silicone Ratio', 'Visual Evaluation', 'Form')) %>% 
  filter (`Silicone Ratio` != 'NA') %>%  
  ggparcoord(columns = c(3,2,4,5), alphaLines = 1,
             groupColumn = 1) +
  geom_vline(xintercept = 1:5, color = "lightblue")+
  ggtitle('Parallel Coordinate Plot for Characteristics by Machine ')
```
Remember we found the in the beginning that Variable and Attribute Exceptions may have correlations between each other. So we picked the top two Variable and Attribute Characteristics to plot the Parallel Coordinate Plot based on the incident count of each of them by machine.  
We found that Part Weight have positive correlation with Visual Evaluation.
Visual Evaluation have positive correlation with Form. 
Silicone do not have strong relationship with other characteristics.

This tells us that there are strong correlation between the top two Attribute Exceptions themselves. And Part Weight, the most significant Variable Exception, has strong correlation with the top Attribute Exception - Visual Evaluation. So it validates our assumption.

That means, if there is a unstable process in any machine, it is likely that it will affect multiple characteristics of the item in a negative way. So a red flag should be raised during Quality inspections that if one of the above characteristics is in exception, the others should be carefully checked as well. 


## Multiple boxplots of Part Weight Result according to WorkcenterDesc Type
```{r}
p6 <- dfn_variable  %>% 
  filter(MIC_Desc == 'Part Weight') %>% 
  ggplot(aes(x=reorder(WorkcenterDesc, Result, median),y=Result)) +
  geom_boxplot() +
  coord_flip() +
  ggtitle("Multiple boxplots of Part Weight Result according to Machine") +
  labs(x="Machine", y="Part Weight")

p6
```
From the Box plots of the part weight data by machine above, we found that there are some machines that run a wider range of part weights. For example, machine I5 and I8. That means the items run on those machines can be varied from small to large sizes. In the opposite, some machines like I9 and I2 run smaller scale of items which have closer weights. 

This plot also tells us it is important to study the Part weight difference ratio from target, which eliminates the impact of the original small or large part weight of the items themselves. And focus on the difference of the part weight with their target to study the level of out-of-range. 

## Ridgeline plot for part weight exception for each line: use weight difference = (result - target)/target.
```{r}
dfn_weight <- dplyr::filter(dfn_variable, MIC_Desc =="Part Weight")
dfn_weight <- within(dfn_weight, weight_diff <- (Result - Target)/Target )
p5 <- ggplot(dfn_weight, aes(x = weight_diff, y = WorkcenterDesc)) +
  geom_density_ridges(fill = "blue") +
  ggtitle("Part Weight Result Difference Ratio from Target by Machine") +
  ylab("Machine")

p5

```
In order to study the difference ratio of Part Weight from Target, we define it by the equation (Result - Target)/Target, and plot that result using the ridge line plot by machine. The Plot above tells us I9 has more out of range Part Weight on the upper side, meaning that they run more over weight. Which is not cost effective to the company. And machine I6, I5, I13, I1 tend to run out of range on the lower bound. So it is important to pay attention to those lines that the quality of products are not compromised because of the low weight. 


## Pick machine I4 and I7 to study because they have most exceptions.
Next, after we study the different patterns on all machines, we would like to take the two machines that we picked in the beginning, I4 and I7, to deep dive into them and find out more valuable information.

### Heatmap
```{r}
dfn_variable7 <- within(dfn_variable7, result_diff <- (Result - Target)/Target )
dfn_variable7 <- dplyr::filter(dfn_variable7, MIC_Desc!="Silicone Application Check")
dfn_variable7 <- dplyr::filter(dfn_variable7, Material_Code!="51800B75")
dfn_variable7w <- dplyr::filter(dfn_variable7, MIC_Desc=="Part Weight")

ggplot(dfn_variable7w, aes(MIC_Desc,Material_Code)) +
  geom_tile(aes(fill = result_diff)) +
  ggtitle("Heatmap for I7 Part Weight Group by Inspection Type") +  labs (y = "Item Number") +
  facet_wrap(~InspectionPlan)


dfn_variable4 <- within(dfn_variable4, result_diff <- (Result - Target)/Target )
dfn_variable4w <- dplyr::filter(dfn_variable4, MIC_Desc=="Part Weight")
ggplot(dfn_variable4w, aes(MIC_Desc,Material_Code)) +
  geom_tile(aes(fill = result_diff))+
  ggtitle("Heatmap for I4 Part Weight Group by Inspection Type") +  labs (y = "Item Number") +
  facet_wrap(~InspectionPlan)


ggplot(dfn_variable7, aes(MIC_Desc,Material_Code)) +
  geom_tile(aes(fill = result_diff))+
  ggtitle("Heatmap for I7 Variable Characteristics") +  labs (y = "Item Number") +
  scale_fill_viridis_c(direction=1)


ggplot(dfn_variable4, aes(MIC_Desc,Material_Code)) +
  geom_tile(aes(fill = result_diff))+
  ggtitle("Heatmap for I4 Variable Characteristics") +  labs (y = "Item Number") +
  scale_fill_viridis_c(direction=1)

```
There are three different types of inspection, which are inspections conducted by different group of people.
We would like to use heatmaps to find out relationship of inspections conducted by different people. The first two graphs represents that relationship of two machines I7 and I4. As shown in the heat map, the part weight collected by different group of people for the same item do not show exact same pattern. They are somewhat similar on color, however, not all items are perfectly the same. Our recommendation for this is to control and align the data collection methods used by different groups of people, in order to make sure data collected are accurate and reflect consistence. 

The third and fourth graphs are to examine the relationship among Variable Exceptions for I7 and I4, where we do not find very similar patterns among those different Variable characteristics. 

### Scatter Plot

```{r}
dfn_variable <- within(dfn_variable, date <- as.Date(PostingDate,'%m/%d/%Y'))
dfn_variable <- within(dfn_variable, result_diff <- (Result - Target)/Target)
dfn_variable47 <- dplyr::filter(dfn_variable, WorkcenterDesc =="I4"|WorkcenterDesc =="I7")
tidy_dfn_variable <- pivot_wider(dfn_variable47, names_from = MIC_Desc, values_from = result_diff)[ , c("WorkcenterDesc","date","Part Weight","Silicone Ratio")]
colnames(tidy_dfn_variable)[3] ="weight"
colnames(tidy_dfn_variable)[4] ="silicone"

weight_silicone <- tidy_dfn_variable %>% 
  group_by(WorkcenterDesc,date) %>% 
  summarise(.,weight= mean(weight,na.rm= TRUE),
            silicone = mean(silicone,na.rm=TRUE))
weight_silicone <- within(weight_silicone, month <- months(date))
weight_silicone <- dplyr::filter(weight_silicone, month!="December")
ggplot(weight_silicone, aes(weight, silicone, color = month)) + 
  geom_point() +
  facet_wrap(~WorkcenterDesc)+
  labs(x = "Part Weight", y = "Silicone Ratio") +
  ggtitle("Machine I4 and I7 Scatter Plot by Part Weight and Silicone Ratio")

```

In order to further study if there is relationship between Part Weight and Silicone Ratio for I4 and I7, we used scatter plots. On I4 there is a weak positive correlation between, however I7 has no strong relationship. This shows that part weight and silicone ratio are not correlated to each other, though they are the top two Variable exceptions. 


### Time series plot of attribute exception throughout the 3 months. For I4 and I7 repectively.  (overall + by week) 

```{r}
var4_time <- dfn_variable4 %>%
  filter(MIC_Desc == 'Part Weight')%>%  
  mutate(Date = as.Date(PostingDate, "%m/%d/%Y"))%>%  
  mutate(ResultDiff = (Result - Target)/Target)%>%
  group_by(Date)%>%
  summarize(MeanRetDiff = mean(ResultDiff))


g <- ggplot(var4_time,aes(Date, MeanRetDiff)) +
  geom_line(alpha = 1) +
  ggtitle("Part Weight Diff Ratio of I4") +  labs (x = "Date", y = "Result") +
  theme_grey(16) +
  theme(legend.title = element_blank())

weekend <- var4_time %>% filter(wday(Date) == 6) 

p11 <- g+geom_point(data = weekend, aes(Date, MeanRetDiff), color = "deeppink")

p11
```

```{r}
p12 <-ggplot(var4_time, aes(Date, MeanRetDiff)) +  
    geom_line(color = "grey30") + geom_point(size = 1) +  
    ggtitle("Part Weight Diff Ratio of I4") +  labs (x = "Date", y = "Result") +
    facet_grid(.~wday(Date, label = TRUE)) +  
    geom_smooth(se = FALSE)

p12

```

Next we would like to study if time has any impact on product performance. So we use time series plot to explore if pattern exists. For machine I4, the first graph is an overview of three month time series. We mark out the weekends with red dot. To see if the more fluctuation exist on weekends comparing to weekdays. The first plot does show the red dots fall on high or low peaks, which validate our assumption. The second graph group the data by days in week and we observe on Saturday, there are more extreme values and wider range. This shows us on machine I4, due to people resource limitation on weekends, the weight exception is more extreme than other days. We recommend the Quality department to pay higher attention on weekends on product and process stability, to avoid quality defects.




```{r}
var7_time <- dfn_variable7 %>%
  filter(MIC_Desc == 'Part Weight')%>%  
  mutate(Date = as.Date(PostingDate, "%m/%d/%Y"))%>%  
  mutate(ResultDiff = (Result - Target)/Target)%>%
  group_by(Date)%>%
  summarize(MeanRetDiff = mean(ResultDiff))


g <- ggplot(var7_time,aes(Date, MeanRetDiff)) +
  geom_line(alpha = 1) +
  ggtitle("Part Weight Diff Ratio of I7") +  labs (x = "Date", y = "Result") +
  theme_grey(16) +
  theme(legend.title = element_blank())

weekend <- var7_time %>% filter(wday(Date) == 6 ) 

p13 <- g+geom_point(data = weekend, aes(Date, MeanRetDiff), color = "deeppink")
p13
```

```{r}
p14 <- ggplot(var7_time, aes(Date, MeanRetDiff)) +  
    geom_line(color = "grey30") + geom_point(size = 1) +
    ggtitle("Part Weight Diff Ratio of I7") +  labs (x = "Date", y = "Result") +
    facet_grid(.~wday(Date, label = TRUE)) +  
    geom_smooth(se = FALSE)

p14
```
We used the same idea to study weekend performance for machine I7, it does not show the same conclusion as machine I4. So that means the people factor is less affected by the day in week on I7.


### Cleveland dot plot with multiple dots
```{r}
dfn_variable4 %>%
  filter(MIC_Desc == 'Part Weight')%>%  
  mutate(Date = as.Date(PostingDate, "%m/%d/%Y"))%>%  
  mutate(ResultDiff = (Result - Target)/Target)%>%
  mutate(month = months(Date))%>%
  filter(month != 'December')%>%
  group_by(Material_Desc, month)%>%
  summarize(MeanRetDiff = mean(ResultDiff))%>%
ggplot(aes(x = MeanRetDiff, y =   fct_reorder2(Material_Desc, month == 'September', MeanRetDiff,.desc=FALSE), color = month)) +

  geom_point()+
  ggtitle("Part Weight Diff Ratio for I4 by Item") +
  ylab("") +
  theme_linedraw()
```
To further dive into to the Part Weight performance for I4, we used Cleveland dot plot to draw the rank of each item run on this machine by Part Weight difference ratio. Based on this graph, we recommend red flagging the top 7 items and low 6 items in the graph on part weight control in order to achieve better part weight performance for the machine overall. 


```{r}
dfn_variable7 %>%
  filter(MIC_Desc == 'Part Weight')%>%  
  mutate(Date = as.Date(PostingDate, "%m/%d/%Y"))%>%  
  mutate(ResultDiff = (Result - Target)/Target)%>%
  mutate(month = months(Date))%>%
  filter(month != 'December')%>%
  group_by(Material_Desc, month)%>%
  summarize(MeanRetDiff = mean(ResultDiff))%>%
ggplot(aes(x = MeanRetDiff, y =   fct_reorder2(Material_Desc, month == 'September', MeanRetDiff,.desc=FALSE), color = month)) +

  geom_point()+
  ggtitle("Part Weight Diff Ratio for I4 by Item") +
  ylab("") +
  theme_linedraw()
```
Use the same logic as above and based on this graph, we recommend red flagging the top 8 items and low 7 items in the graph on part weight control in order to achieve better part weight performance for machine I7 overall. 

Also in the above two graphs, we see that for items that show up in more than two months, the weight of them tend to increase by time, which shows it could potentially be affected by the tool of the item being fatigue and require maintenance over time. 

<!--chapter:end:results.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Interactive component



<!--chapter:end:interactive.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Conclusion


<!--chapter:end:conclusion.Rmd-->

